{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fd = Path('./dataset/')\n",
    "AL_train = ds_fd/'AL/train'\n",
    "EN_train = ds_fd/'EN/train'\n",
    "SG_train = ds_fd/'SG/train'\n",
    "CN_train = ds_fd/'CN/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        X, Y = [], []\n",
    "        temp_x, temp_y = [], []\n",
    "        for x in f:\n",
    "            if x != '\\n':\n",
    "                x, y = x.split(' ')\n",
    "                y = y.strip('\\n')\n",
    "                temp_x.append(x)\n",
    "                temp_y.append(y)\n",
    "            else:\n",
    "                X.append(temp_x)\n",
    "                Y.append(temp_y)\n",
    "                temp_x, temp_y = [], []\n",
    "    return X, Y       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tags(Y):\n",
    "    tags = [t for y in Y for t in y]\n",
    "    return list(set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(X):\n",
    "    words = [w for x in X for w in x]\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Emission Parameters\n",
    "\n",
    "The emission matrix is in the form of (tag, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_para(X, Y):\n",
    "    \n",
    "    y = [t for sublist in Y for t in sublist]\n",
    "    x = [w for sublist in X for w in sublist]\n",
    "    \n",
    "    words = get_unique_words(X)\n",
    "    tags = get_unique_tags(Y)\n",
    "    \n",
    "    word2index = {w:i for i,w in enumerate(words)}\n",
    "    tag2index = {t:i for i,t in enumerate(tags)}\n",
    "        \n",
    "    counts = np.zeros((len(tags), len(words)))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        counts[tag2index[y[i]], word2index[x[k]]] += 1\n",
    "    \n",
    "    emission_prob = counts/np.sum(counts,1)[:,None]\n",
    "    \n",
    "    return emission_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Emission Parameters by Smoothing\n",
    "\n",
    "Based on the get_emission_para()\n",
    "\n",
    "First count sum of all the words under k appearances, set it as the counts for '#UNK#'\n",
    "\n",
    "Second delete columns\n",
    "\n",
    "third update vocabulary dictionary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_para_smoothing(X, Y, K):\n",
    "    \n",
    "    y = [t for sublist in Y for t in sublist]\n",
    "    x = [w for sublist in X for w in sublist]\n",
    "    \n",
    "    words = get_unique_words(X)\n",
    "    tags = get_unique_tags(Y)\n",
    "    \n",
    "    word2index = {w:i for i,w in enumerate(words)}\n",
    "    tag2index = {t:i for i,t in enumerate(tags)}\n",
    "    index2tag = {i:t for i,t in enumerate(tags)}\n",
    "        \n",
    "    counts = np.zeros((len(tags), len(words)))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        counts[tag2index[y[i]], word2index[x[i]]] += 1\n",
    "    \n",
    "    removed_index = np.sum(counts, 0) < K\n",
    "\n",
    "    if (np.sum(removed_index) > 0):\n",
    "        \n",
    "        counts = np.append(counts, np.sum(counts[:, removed_index], 1)[:,None], 1)\n",
    "        counts = np.delete(counts, np.nonzero(removed_index), 1)\n",
    "        \n",
    "        new_words = [words[j] for j in range(len(words)) if not removed_index[j]]\n",
    "        word2index = {w:i for i,w in enumerate(new_words)}\n",
    "        word2index['#UNK#'] = len(new_words)\n",
    "    \n",
    "    emission_prob = counts/np.sum(counts,1)[:,None]\n",
    "    \n",
    "    return emission_prob, word2index, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_evaluation(word, emission_prob, word2index, index2tag):\n",
    "    if word in vocab:\n",
    "        return index2tag[np.argmax(emission_prob[:,word2index[word]])]\n",
    "    else:\n",
    "        return index2tag[np.argmax(emission_prob[:,word2index['#UNK#']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_dev_in = ds_fd/'AL/dev.in'\n",
    "EN_dev_in = ds_fd/'EN/dev.in'\n",
    "SG_dev_in = ds_fd/'SG/dev.in'\n",
    "CN_dev_in = ds_fd/'CN/dev.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_prediction_output(ds):\n",
    "    \n",
    "    train = ds_fd/(ds + '/train')\n",
    "    dev_in = ds_fd/(ds + '/dev.in')\n",
    "    \n",
    "    X, Y = read_data(train)\n",
    "    emission_prob, word2index, tag2index, index2tag = get_emission_para_smoothing(X, Y, 3)\n",
    "    \n",
    "    with open(dev_in) as f:\n",
    "        input_data = f.readlines()\n",
    "    \n",
    "    output_string = ''\n",
    "    \n",
    "    for instance in input_data:\n",
    "        if (instance != '\\n'):\n",
    "            word = instance.strip('\\n')\n",
    "            tag = naive_evaluation(word, emission_prob, word2index, index2tag)\n",
    "            output_string += word + ' ' + tag + '\\n'\n",
    "        else:\n",
    "            output_string += '\\n'\n",
    "    \n",
    "    dev_out = ds_fd/(ds + '/dev.p2.out')\n",
    "    with open(dev_out, 'w') as f:\n",
    "        f.write(output_string)\n",
    "    \n",
    "    print('Done with writing predictions')\n",
    "    \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Transition Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_para(Y):\n",
    "    \n",
    "    tags = get_unique_tags(Y)\n",
    "    \n",
    "    tag2index = {t:i for i,t in enumerate(tags)}\n",
    "        \n",
    "    counts = np.zeros((len(tags)+1, len(tags)+1))\n",
    "    \n",
    "    for instance in Y:\n",
    "        for k in range(len(instance)-1):\n",
    "            counts[tag2index[instance[k]], tag2index[instance[k+1]]] += 1\n",
    "        counts[-1, tag2index[instance[0]]] += 1\n",
    "        counts[tag2index[instance[-1]], -1] += 1\n",
    "    \n",
    "    transition_prob = counts/np.sum(counts,1)[:,None]\n",
    "    \n",
    "    return transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(sentence, emission_prob, transition_prob, word2index, tag2index, index2tag):\n",
    "    \n",
    "    score_matrix = np.zeros((len(tag2index), len(sentence)))\n",
    "    path_matrix = np.zeros((len(tag2index), len(sentence)), dtype=int)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(tag2index)):\n",
    "            if i == 0:\n",
    "                if sentence[i] in word2index:\n",
    "                    score_matrix[j, i] = transition_prob[-1, j] * emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    score_matrix[j, i] = transition_prob[-1, j] * emission_prob[j, word2index[\"#UNK#\"]]\n",
    "            else:\n",
    "                if sentence[i] in word2index:\n",
    "                    competitors = score_matrix[:, i-1] * transition_prob[:-1, j] * emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    competitors = score_matrix[:, i-1] * transition_prob[:-1, j] * emission_prob[j, word2index[\"#UNK#\"]]\n",
    "                path_matrix[j,i] = np.argmax(competitors)\n",
    "                score_matrix[j,i] = np.max(competitors)\n",
    "    \n",
    "    last_parent_node = np.argmax(score_matrix[:, -1] * transition_prob[:-1, -1])\n",
    "    \n",
    "    path = [last_parent_node]\n",
    "    for m in range(len(sentence)-1, 0, -1):\n",
    "        path.insert(0, path_matrix[path[0], m])\n",
    "    output_tags = []\n",
    "    for n in path:\n",
    "        output_tags.append(index2tag[n])\n",
    "    \n",
    "    return output_tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum of log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(sentence, emission_prob, transition_prob, word2index, tag2index, index2tag):\n",
    "    \n",
    "    transition_prob = np.ma.log(transition_prob).filled(-np.inf)\n",
    "    emission_prob = np.ma.log(emission_prob).filled(-np.inf)\n",
    "    \n",
    "    score_matrix = np.zeros((len(tag2index), len(sentence)))\n",
    "    path_matrix = np.zeros((len(tag2index), len(sentence)), dtype=int)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(tag2index)):\n",
    "            if i == 0:\n",
    "                if sentence[i] in word2index:\n",
    "                    score_matrix[j, i] = transition_prob[-1, j] + emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    score_matrix[j, i] = transition_prob[-1, j] + emission_prob[j, word2index[\"#UNK#\"]]\n",
    "            else:\n",
    "                if sentence[i] in word2index:\n",
    "                    competitors = score_matrix[:, i-1] + transition_prob[:-1, j] + emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    competitors = score_matrix[:, i-1] + transition_prob[:-1, j] + emission_prob[j, word2index[\"#UNK#\"]]\n",
    "                path_matrix[j,i] = np.argmax(competitors)\n",
    "                score_matrix[j,i] = np.max(competitors)\n",
    "    last_parent_node = np.argmax(score_matrix[:, -1] + transition_prob[:-1, -1])\n",
    "    \n",
    "    path = [last_parent_node]\n",
    "    for m in range(len(sentence)-1, 0, -1):\n",
    "        path.insert(0, path_matrix[path[0], m])\n",
    "    output_tags = []\n",
    "    for n in path:\n",
    "        output_tags.append(index2tag[n])\n",
    "    \n",
    "    return output_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm output in stipulated format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_output(ds):\n",
    "    \n",
    "    train = ds_fd/(ds + '/train')\n",
    "    dev_in = ds_fd/(ds + '/dev.in')\n",
    "    \n",
    "    X, Y = read_data(train)\n",
    "    emission_prob, word2index, tag2index, index2tag = get_emission_para_smoothing(X, Y, 3)\n",
    "    transition_prob = get_transition_para(Y)\n",
    "    \n",
    "    sentences, sentence = [], []\n",
    "    \n",
    "    with open(dev_in) as f:\n",
    "        for line in f:\n",
    "            if (line != '\\n'):\n",
    "                word = line.strip('\\n')\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    tags = [viterbi_algo(sentence, emission_prob, transition_prob, word2index, tag2index, index2tag) for sentence in sentences]\n",
    "    \n",
    "    output_string = ''\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            output_string += sentences[i][j] + ' ' + tags[i][j] + '\\n'\n",
    "        output_string += '\\n'\n",
    "    \n",
    "    dev_out = ds_fd/(ds + '/dev.p3.out')\n",
    "    with open(dev_out, 'w') as f:\n",
    "        f.write(output_string)\n",
    "    \n",
    "    print('Done with writing predictions')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "viterbi_output('AL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "viterbi_output('CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "viterbi_output('SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "viterbi_output('EN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm sum of log result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8556\r\n",
      "\r\n",
      "#Correct Entity : 6725\r\n",
      "Entity  precision: 0.7860\r\n",
      "Entity  recall: 0.7998\r\n",
      "Entity  F: 0.7929\r\n",
      "\r\n",
      "#Correct Sentiment : 6073\r\n",
      "Sentiment  precision: 0.7098\r\n",
      "Sentiment  recall: 0.7223\r\n",
      "Sentiment  F: 0.7160\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/AL/dev.out dataset/AL/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 13529\r\n",
      "\r\n",
      "#Correct Entity : 10999\r\n",
      "Entity  precision: 0.8130\r\n",
      "Entity  recall: 0.8346\r\n",
      "Entity  F: 0.8236\r\n",
      "\r\n",
      "#Correct Sentiment : 10314\r\n",
      "Sentiment  precision: 0.7624\r\n",
      "Sentiment  recall: 0.7826\r\n",
      "Sentiment  F: 0.7724\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/EN/dev.out dataset/EN/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 3054\n",
      "\n",
      "#Correct Entity : 1662\n",
      "Entity  precision: 0.5442\n",
      "Entity  recall: 0.3663\n",
      "Entity  F: 0.4379\n",
      "\n",
      "#Correct Sentiment : 1035\n",
      "Sentiment  precision: 0.3389\n",
      "Sentiment  recall: 0.2281\n",
      "Sentiment  F: 0.2727\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/SG/dev.out dataset/SG/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 770\r\n",
      "\r\n",
      "#Correct Entity : 309\r\n",
      "Entity  precision: 0.4013\r\n",
      "Entity  recall: 0.2091\r\n",
      "Entity  F: 0.2749\r\n",
      "\r\n",
      "#Correct Sentiment : 210\r\n",
      "Sentiment  precision: 0.2727\r\n",
      "Sentiment  recall: 0.1421\r\n",
      "Sentiment  F: 0.1868\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/CN/dev.out dataset/CN/dev.p3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm Simple Product Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8556\r\n",
      "\r\n",
      "#Correct Entity : 6725\r\n",
      "Entity  precision: 0.7860\r\n",
      "Entity  recall: 0.7998\r\n",
      "Entity  F: 0.7929\r\n",
      "\r\n",
      "#Correct Sentiment : 6073\r\n",
      "Sentiment  precision: 0.7098\r\n",
      "Sentiment  recall: 0.7223\r\n",
      "Sentiment  F: 0.7160\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/AL/dev.out dataset/AL/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 13529\r\n",
      "\r\n",
      "#Correct Entity : 10999\r\n",
      "Entity  precision: 0.8130\r\n",
      "Entity  recall: 0.8346\r\n",
      "Entity  F: 0.8236\r\n",
      "\r\n",
      "#Correct Sentiment : 10314\r\n",
      "Sentiment  precision: 0.7624\r\n",
      "Sentiment  recall: 0.7826\r\n",
      "Sentiment  F: 0.7724\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/EN/dev.out dataset/EN/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4537\r\n",
      "#Entity in prediction: 3054\r\n",
      "\r\n",
      "#Correct Entity : 1662\r\n",
      "Entity  precision: 0.5442\r\n",
      "Entity  recall: 0.3663\r\n",
      "Entity  F: 0.4379\r\n",
      "\r\n",
      "#Correct Sentiment : 1035\r\n",
      "Sentiment  precision: 0.3389\r\n",
      "Sentiment  recall: 0.2281\r\n",
      "Sentiment  F: 0.2727\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/SG/dev.out dataset/SG/dev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 773\r\n",
      "\r\n",
      "#Correct Entity : 309\r\n",
      "Entity  precision: 0.3997\r\n",
      "Entity  recall: 0.2091\r\n",
      "Entity  F: 0.2745\r\n",
      "\r\n",
      "#Correct Sentiment : 210\r\n",
      "Sentiment  precision: 0.2717\r\n",
      "Sentiment  recall: 0.1421\r\n",
      "Sentiment  F: 0.1866\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/CN/dev.out dataset/CN/dev.p3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Can be seen, since the length of each sentence is not long enough to cause underfloating problem, \n",
    "\n",
    "the results from both methods are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy slicing is not deep copy TMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kth best viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best is to use a heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo_kth_best(sentence, emission_prob, transition_prob, word2index, tag2index, index2tag, K):\n",
    "    \n",
    "    score_matrix = np.zeros((len(tag2index), len(sentence), K))\n",
    "    path_matrix = np.zeros((len(tag2index), len(sentence), K), dtype=int)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(tag2index)):\n",
    "            if i == 0:\n",
    "                if sentence[i] in word2index:\n",
    "                    score_matrix[j, i, 0] += transition_prob[-1, j] * emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    score_matrix[j, i, 0] += transition_prob[-1, j] * emission_prob[j, word2index[\"#UNK#\"]]\n",
    "            else:\n",
    "                transition_vector = transition_prob[:-1, j]\n",
    "                competitors = score_matrix[:, i-1, 0].copy()\n",
    "                track = [0] * len(tag2index)\n",
    "                emission_constant = 0\n",
    "                if sentence[i] in word2index:\n",
    "                    emission_constant = emission_prob[j, word2index[sentence[i]]]\n",
    "                else:\n",
    "                    emission_constant = emission_prob[j, word2index[\"#UNK#\"]]\n",
    "                for k in range(K-1):\n",
    "                    score_matrix[j, i, k] = np.max(competitors * transition_vector * emission_constant)\n",
    "                    selected_index = np.argmax(competitors * transition_vector)\n",
    "                    path_matrix[j, i, k] = selected_index\n",
    "                    track[selected_index] += 1\n",
    "                    competitors[selected_index] = score_matrix[selected_index, i-1, track[selected_index]]\n",
    "                score_matrix[j, i, -1] = np.max(competitors * transition_vector * emission_constant)\n",
    "                path_matrix[j, i, -1] = np.argmax(competitors * transition_vector)\n",
    "    \n",
    "    terminate_vector = transition_prob[:-1, -1]\n",
    "    competitors = score_matrix[:, -1, 0]\n",
    "    track = [0] * len(tag2index)\n",
    "    top_k_nodes = np.zeros(K, dtype=int)\n",
    "    for k in range(K-1):\n",
    "        selected_index = np.argmax(competitors * terminate_vector)\n",
    "        top_k_nodes[k] = selected_index\n",
    "        track[selected_index] += 1\n",
    "        competitors[selected_index] = score_matrix[selected_index, -1, track[selected_index]]\n",
    "    top_k_nodes[-1] = np.argmax(competitors * terminate_vector)\n",
    "    \n",
    "    parent_node = top_k_nodes[-1]\n",
    "    order = np.where(top_k_nodes == parent_node)[0].size\n",
    "    \n",
    "    path = [parent_node]\n",
    "    for m in range(len(sentence)-1, 0, -1):\n",
    "        top_nodes = path_matrix[path[0], m, :order]\n",
    "        parent_node = top_nodes[-1]\n",
    "        path.insert(0, parent_node)\n",
    "        order = np.where(top_nodes == parent_node)[0].size\n",
    "\n",
    "    output_tags = [index2tag[index] for index in path]\n",
    "    \n",
    "    return output_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example used for k-best Viterbi debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_debug = ['the', 'dog', 'cat', 'the', 'cat', 'cat', 'dog', 'the', 'cat']\n",
    "emission_prob_debug = np.array([[0.6, 0.3, 0.1], [0.1, 0.4, 0.5]])\n",
    "transition_prob_debug = np.array([[0.4, 0.4, 0.2], [0.2, 0.6, 0.2], [0.5, 0.5, 0]])\n",
    "word2index_debug = {'the': 0, 'dog': 1, 'cat': 2}\n",
    "tag2index_debug = {'A': 0, 'B': 1}\n",
    "index2tag_debug = {0: 'A', 1: 'B'}\n",
    "K_debug = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_algo_kth_best(sentence_debug, emission_prob_debug, \n",
    "                      transition_prob_debug, word2index_debug, tag2index_debug, index2tag_debug, K_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th-best Viterbi Algorithm output in stipulated format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seventh_best_viterbi_output(ds):\n",
    "    \n",
    "    train = ds_fd/(ds + '/train')\n",
    "    dev_in = ds_fd/(ds + '/dev.in')\n",
    "    \n",
    "    X, Y = read_data(train)\n",
    "    emission_prob, word2index, tag2index, index2tag = get_emission_para_smoothing(X, Y, 3)\n",
    "    transition_prob = get_transition_para(Y)\n",
    "    \n",
    "    sentences, sentence = [], []\n",
    "    \n",
    "    with open(dev_in) as f:\n",
    "        for line in f:\n",
    "            if (line != '\\n'):\n",
    "                word = line.strip('\\n')\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    \n",
    "    tags = [viterbi_algo_kth_best(sentence, emission_prob, transition_prob, word2index, tag2index, index2tag, 7) for sentence in sentences]\n",
    "    \n",
    "    output_string = ''\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            output_string += sentences[i][j] + ' ' + tags[i][j] + '\\n'\n",
    "        output_string += '\\n'\n",
    "    \n",
    "    dev_out = ds_fd/(ds + '/dev.p4.out')\n",
    "    with open(dev_out, 'w') as f:\n",
    "        f.write(output_string)\n",
    "    \n",
    "    print('Done with writing predictions')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "seventh_best_viterbi_output('AL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "seventh_best_viterbi_output('CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "seventh_best_viterbi_output('SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "seventh_best_viterbi_output('EN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of 7th best Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8970\r\n",
      "\r\n",
      "#Correct Entity : 5985\r\n",
      "Entity  precision: 0.6672\r\n",
      "Entity  recall: 0.7118\r\n",
      "Entity  F: 0.6888\r\n",
      "\r\n",
      "#Correct Sentiment : 5013\r\n",
      "Sentiment  precision: 0.5589\r\n",
      "Sentiment  recall: 0.5962\r\n",
      "Sentiment  F: 0.5769\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/AL/dev.out dataset/AL/dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 13810\r\n",
      "\r\n",
      "#Correct Entity : 10444\r\n",
      "Entity  precision: 0.7563\r\n",
      "Entity  recall: 0.7925\r\n",
      "Entity  F: 0.7739\r\n",
      "\r\n",
      "#Correct Sentiment : 9731\r\n",
      "Sentiment  precision: 0.7046\r\n",
      "Sentiment  recall: 0.7384\r\n",
      "Sentiment  F: 0.7211\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/EN/dev.out dataset/EN/dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4537\r\n",
      "#Entity in prediction: 5005\r\n",
      "\r\n",
      "#Correct Entity : 1725\r\n",
      "Entity  precision: 0.3447\r\n",
      "Entity  recall: 0.3802\r\n",
      "Entity  F: 0.3616\r\n",
      "\r\n",
      "#Correct Sentiment : 877\r\n",
      "Sentiment  precision: 0.1752\r\n",
      "Sentiment  recall: 0.1933\r\n",
      "Sentiment  F: 0.1838\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/SG/dev.out dataset/SG/dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 1198\r\n",
      "\r\n",
      "#Correct Entity : 315\r\n",
      "Entity  precision: 0.2629\r\n",
      "Entity  recall: 0.2131\r\n",
      "Entity  F: 0.2354\r\n",
      "\r\n",
      "#Correct Sentiment : 199\r\n",
      "Sentiment  precision: 0.1661\r\n",
      "Sentiment  recall: 0.1346\r\n",
      "Sentiment  F: 0.1487\r\n"
     ]
    }
   ],
   "source": [
    "! python3 dataset/EvalScript/evalResult.py dataset/CN/dev.out dataset/CN/dev.p4.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
