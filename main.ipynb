{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "### Hong Pengfei               \n",
    "### Gao Yunyi \n",
    "### Wu Tianyu\n",
    " -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PGM.GM.HMM.hidden_markov_model import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input data is stored in List[List[str]] where: <br>\n",
    "#### for sentences:  each string is one word, each sublist is one sentence<br>\n",
    "#### for label: each string is one label, each sublist is labels for one sentence in corresponding sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('./dataset/')\n",
    "AL = DATA_FOLDER/'AL'\n",
    "AL_train = AL/'train'\n",
    "AL_dev_x = AL/'dev.in'\n",
    "AL_dev_y = AL/'dev.out'\n",
    "AL_out_2 = AL/'dev.p2.out'\n",
    "AL_out_3 = AL/'dev.p3.out'\n",
    "AL_out_4 = AL/'dev.p4.out'\n",
    "AL_crf_1 = AL/'dev.crf1.out'\n",
    "Al_crf_2 = AL/'dev.crf2.out'\n",
    "AL_memm_1 = AL/'dev.memm1.out'\n",
    "AL_sp_1 = AL/'dev.sp1.out'\n",
    "\n",
    "\n",
    "EN = DATA_FOLDER/'EN'\n",
    "EN_train = EN/'train'\n",
    "EN_dev_x = EN/'dev.in'\n",
    "EN_dev_y = EN/'dev.out'\n",
    "EN_out_2 = EN/'dev.p2.out'\n",
    "EN_out_3 = EN/'dev.p3.out'\n",
    "EN_out_4 = EN/'dev.p4.out'\n",
    "EN_crf_1 = EN/'dev.crf1.out'\n",
    "EN_memm_1 = EN/'dev.memm1.out'\n",
    "EN_sp_1 = EN/'dev.sp1.out'\n",
    "\n",
    "CN = DATA_FOLDER/'CN'\n",
    "CN_train = CN/'train'\n",
    "CN_dev_x = CN/'dev.in'\n",
    "CN_dev_y = CN/'dev.out'\n",
    "CN_out_2 = CN/'dev.p2.out'\n",
    "CN_out_3 = CN/'dev.p3.out'\n",
    "\n",
    "SG = DATA_FOLDER/'SG'\n",
    "SG_train = SG/'train'\n",
    "SG_dev_x = SG/'dev.in'\n",
    "SG_dev_y = SG/'dev.out'\n",
    "SG_out_2 = SG/'dev.p2.out'\n",
    "SG_out_3 = SG/'dev.p3.out'\n",
    "\n",
    "PARA_FOLDER = DATA_FOLDER/'parameter'\n",
    "AL_para_FOLDER = PARA_FOLDER/'AL'\n",
    "EN_para_FOLDER = PARA_FOLDER/'EN'\n",
    "\n",
    "\n",
    "EVAL_script = './EvalScript/evalResult.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count is stored in a $|V|\\times|T|$ matrix\n",
    "\n",
    "#### The emission probabilities are obtained by dividing each entry in the counting matrix by the sum of corresponding row\n",
    "\n",
    "#### The prediction algorithm is simple: find the maximum entry's column index and convert it to tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 19484\r\n",
      "\r\n",
      "#Correct Entity : 2898\r\n",
      "Entity  precision: 0.1487\r\n",
      "Entity  recall: 0.3447\r\n",
      "Entity  F: 0.2078\r\n",
      "\r\n",
      "#Correct Sentiment : 2457\r\n",
      "Sentiment  precision: 0.1261\r\n",
      "Sentiment  recall: 0.2922\r\n",
      "Sentiment  F: 0.1762\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(AL_dev_y, AL_out_2)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 19406\r\n",
      "\r\n",
      "#Correct Entity : 9152\r\n",
      "Entity  precision: 0.4716\r\n",
      "Entity  recall: 0.6944\r\n",
      "Entity  F: 0.5617\r\n",
      "\r\n",
      "#Correct Sentiment : 7644\r\n",
      "Sentiment  precision: 0.3939\r\n",
      "Sentiment  recall: 0.5800\r\n",
      "Sentiment  F: 0.4692\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(EN_dev_y, EN_out_2)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4537\r\n",
      "#Entity in prediction: 18451\r\n",
      "\r\n",
      "#Correct Entity : 2632\r\n",
      "Entity  precision: 0.1426\r\n",
      "Entity  recall: 0.5801\r\n",
      "Entity  F: 0.2290\r\n",
      "\r\n",
      "#Correct Sentiment : 1239\r\n",
      "Sentiment  precision: 0.0672\r\n",
      "Sentiment  recall: 0.2731\r\n",
      "Sentiment  F: 0.1078\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(SG_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(SG_dev_y, SG_out_2)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 9373\r\n",
      "\r\n",
      "#Correct Entity : 765\r\n",
      "Entity  precision: 0.0816\r\n",
      "Entity  recall: 0.5176\r\n",
      "Entity  F: 0.1410\r\n",
      "\r\n",
      "#Correct Sentiment : 285\r\n",
      "Sentiment  precision: 0.0304\r\n",
      "Sentiment  recall: 0.1928\r\n",
      "Sentiment  F: 0.0525\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(CN_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(CN_dev_y, CN_out_2)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <SOS\\> and <EOS\\> are added at the beginning and end of sentence respectively.\n",
    "\n",
    "#### Traverse through each sentence. \n",
    "\n",
    "#### The count is stored in a $(|T|+1)\\times(|T|+1)$ matrix. The rows are initial state and columns are the state trasitted to.\n",
    "\n",
    "#### The probabilities are calculated by dividing each entry with the sum of correspodning row.\n",
    "\n",
    "#### For Viterbi Algorithm, one parent matrix and one score matrix of size $n\\times(|T|+1)$ are implemented\n",
    "\n",
    "#### The optimal path is restored by finding the argmax among $|T|$ tags in previous position\n",
    "\n",
    "#### In our implementation, the code snippet is the same as Part 4 since Part 4 addresses a more general problem.\n",
    "\n",
    "#### To make use of code snippet for Part 4, simply set k=1 for the optimal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8475\r\n",
      "\r\n",
      "#Correct Entity : 6717\r\n",
      "Entity  precision: 0.7926\r\n",
      "Entity  recall: 0.7989\r\n",
      "Entity  F: 0.7957\r\n",
      "\r\n",
      "#Correct Sentiment : 6068\r\n",
      "Sentiment  precision: 0.7160\r\n",
      "Sentiment  recall: 0.7217\r\n",
      "Sentiment  F: 0.7188\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(AL_dev_y, AL_out_3, k=1)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 12723\r\n",
      "\r\n",
      "#Correct Entity : 10791\r\n",
      "Entity  precision: 0.8481\r\n",
      "Entity  recall: 0.8188\r\n",
      "Entity  F: 0.8332\r\n",
      "\r\n",
      "#Correct Sentiment : 10378\r\n",
      "Sentiment  precision: 0.8157\r\n",
      "Sentiment  recall: 0.7875\r\n",
      "Sentiment  F: 0.8013\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(EN_dev_y, EN_out_3, k=1)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4537\r\n",
      "#Entity in prediction: 3036\r\n",
      "\r\n",
      "#Correct Entity : 1662\r\n",
      "Entity  precision: 0.5474\r\n",
      "Entity  recall: 0.3663\r\n",
      "Entity  F: 0.4389\r\n",
      "\r\n",
      "#Correct Sentiment : 1035\r\n",
      "Sentiment  precision: 0.3409\r\n",
      "Sentiment  recall: 0.2281\r\n",
      "Sentiment  F: 0.2733\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(SG_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(SG_dev_y, SG_out_3, k=1)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 769\r\n",
      "\r\n",
      "#Correct Entity : 309\r\n",
      "Entity  precision: 0.4018\r\n",
      "Entity  recall: 0.2091\r\n",
      "Entity  F: 0.2750\r\n",
      "\r\n",
      "#Correct Sentiment : 210\r\n",
      "Sentiment  precision: 0.2731\r\n",
      "Sentiment  recall: 0.1421\r\n",
      "Sentiment  F: 0.1869\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(CN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(CN_dev_y, CN_out_3, k=1)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Top 7th sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parent 'matrix' and score 'matrix' are implemented as a numpy 3-d array of size $(n+2)\\times(|T|+1)\\times K$.\n",
    "\n",
    "#### The new dimension stores the top K scores and corresponding parent nodes for all positions and tags\n",
    "\n",
    "#### How to backtrack K-th best path:\n",
    "\n",
    "#### Repetition begins: select the parent tags for the first K paths for the <EOS\\> position and identify the parent tag 'T' corresponding to the K-th best path among all $|T|\\times K$ possible paths\n",
    "\n",
    "#### Count the number of apperances of 'T' in the K parent tags for the first K paths.  Call this number ORDER\n",
    "\n",
    "#### Repeat the repetitive steps describe above for one position before, this time select first ORDER instead of K\n",
    "\n",
    "#### Terminate until recovering the entire path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8932\r\n",
      "\r\n",
      "#Correct Entity : 5980\r\n",
      "Entity  precision: 0.6695\r\n",
      "Entity  recall: 0.7112\r\n",
      "Entity  F: 0.6897\r\n",
      "\r\n",
      "#Correct Sentiment : 5014\r\n",
      "Sentiment  precision: 0.5614\r\n",
      "Sentiment  recall: 0.5963\r\n",
      "Sentiment  F: 0.5783\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(AL_dev_y, AL_out_4, k=7)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_out_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 13153\r\n",
      "\r\n",
      "#Correct Entity : 10232\r\n",
      "Entity  precision: 0.7779\r\n",
      "Entity  recall: 0.7764\r\n",
      "Entity  F: 0.7772\r\n",
      "\r\n",
      "#Correct Sentiment : 9719\r\n",
      "Sentiment  precision: 0.7389\r\n",
      "Sentiment  recall: 0.7375\r\n",
      "Sentiment  F: 0.7382\r\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(EN_dev_y, EN_out_4, k=7)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_out_4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 1 : HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of HMM:\n",
    "1. learns distribution between p(x, y), performs generally better when there is an small dataset\n",
    "\n",
    "### Disadvantage of HMM:\n",
    "\n",
    "2. HMM is a generative model modelling the distributon of x,y at the same time. When the distribution of x for training dataset is different from the distribution of x for testing dataset (which is very likely), the performance on testing dataset would be very poor.\n",
    "\n",
    "3. Features of input x is ignored in the model. In NLP tasks, the variety of features in languages is discarded and ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have found that after adding the smoothing to the parameters, all the performances have increased up by 2~3 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Entity  F: 0.8596\n",
    "\n",
    "\n",
    "Sentiment  F: 0.7956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity  F: 0.8157\n",
    "\n",
    "\n",
    "Sentiment  F: 0.7288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 2 : MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advatange of MEMM:\n",
    "\n",
    "1. Discriminative model, solving the issue mentioned in disadvantage of HMM\n",
    "\n",
    "2. Flexible for different feature functions, mining language features\n",
    "\n",
    "### Disadvantage of MEMM:\n",
    "\n",
    "Labelling bias: MEMM sometimes cannot capture the general trend of state transitions because it adopts local variance normalization. The local optimal solution is achieved but it might not be the globally optimal one. The model is limited by this issue, especially when the numbers of convertible states vary significantly among states. Too many convertible states diluted the transition probability, resulting in unequal optimal solution and global solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_MEMM_1=EN_para_FOLDER/'MEMM_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_MEMM_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "memm = DM.get_MEMM(datafile, modelfile, regularization)\n",
    "memm.load_model(modelfile)\n",
    "memm.load_data()\n",
    "memm.train(epoch,lr=0.0.05,start_epoch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity  F: 0.8330\n",
    "\n",
    "Sentiment  F: 0.7599"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_MEMM_1=AL_para_FOLDER/'MEMM_1'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_MEMM_1\n",
    "regularization = 10\n",
    "epoch = 3\n",
    "memm = DM.get_MEMM(datafile, modelfile, regularization)\n",
    "memm.load_data()\n",
    "memm.train(epoch,lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = memm.get_SP(datafile, modelfile, regularization)\n",
    "memm.load_model(modelfile)\n",
    "memm.test(AL_dev_y, AL_memm_1)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_memm_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity  F: 0.7833\n",
    "\n",
    "Sentiment  F: 0.7299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, we move on from MEMM to CRF, which addresses the labelling bias problem perfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 3 : CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of CRF:\n",
    "\n",
    "1. Compared to HMM: There is no Markov assumptions, which is sometimes unreasonable.\n",
    "\n",
    "2. Compared to MEMM: Global variance normalization is adopted. Global trend is captured. Labelling bias problem is addressed.\n",
    "\n",
    "3. Feature functions are flexible, same as MEMM\n",
    "\n",
    "### Disadvantage of CRF:\n",
    "\n",
    "1. Computationally complicated. It takes our model hours and hours to train. This also results in low flexibility when new training data is availble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation, we have tried: \n",
    "1. different Regularization constants sigma square to prevent overfiting for CRF, we found 1/10 ~ 1/20 to be a good range.\n",
    "\n",
    "2. different feature functions to extract the most information out of the word. for example, we have tried encode the capitalization of the first word as a feature and lower case everything. we also tried encode numbers into a fixed symbol NUM to deal with every number as a key word. and we use a vocabulary of the words appear more than two times, all of which helped as an extra function.\n",
    "\n",
    "3. we also tried to smooth the weights by adding a small constant to the weights, and we found that can be useful.\n",
    "\n",
    "4. we have also tried different window sizes of 1, 2, 3, 4, 5 to encode adjcent word information. we found that a window size of 2 is of great help.\n",
    "\n",
    "\n",
    "\n",
    "however, due to the limitation, we are not able to fully exploit the search space for all the hyperparameter matching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training implementation details:\n",
    "1. we used everything in log space to prevent overflow problems. specifically we also use a special trick in mathematics to deal with overflow problems in calculating log-sum-exp function\n",
    "2. to replace the lbfgs function in scipy, we have tried to train using gradient descent, we have tried differnet methods to quickly update the parameters. allowing learning rate to decay with the number of epochs increase. \n",
    "3. we also tried implementing CRF batching training to save time. but did not manage to properly write the code.\n",
    "4. we tried using a small number of samples to update the learning rate, but it did not help as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_CRF_1=EN_para_FOLDER/'CRF_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_CRF_1\n",
    "regularization = 10\n",
    "epoch = 10\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.load_data()\n",
    "crf.train(epoch,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.test(EN_dev_y,EN_crf_1)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_crf_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity  F: 0.9094\n",
    "\n",
    "Sentiment  F: 0.88884"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_CRF_2=AL_para_FOLDER/'CRF_2'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_CRF_2\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_data()\n",
    "crf.train(epoch,lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "******** CRF *********\n",
    "* Number of labels: 42\n",
    "* Number of features: 443993\n",
    "* Initialized weight of size: 443993\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: 653901.5620625856\n",
    "   Iteration: 1, Negative Log-likelihood: 71151.57979157224\n",
    "   Iteration: 2, Negative Log-likelihood: 134826.53628937004\n",
    "   Iteration: 3, Negative Log-likelihood: 35996.291548169036\n",
    "   Iteration: 4, Negative Log-likelihood: 50068.4311422095\n",
    "   Iteration: 5, Negative Log-likelihood: 35184.22091465483\n",
    "   Iteration: 6, Negative Log-likelihood: 29531.13816272047\n",
    "   Iteration: 7, Negative Log-likelihood: 25449.639322340125\n",
    "   Iteration: 8, Negative Log-likelihood: 20163.900435345706\n",
    "   Iteration: 9, Negative Log-likelihood: 19568.620540475848\n",
    "   Iteration: 10, Negative Log-likelihood: 15701.300994498764\n",
    "   Iteration: 11, Negative Log-likelihood: 16576.333405049903\n",
    "   Iteration: 12, Negative Log-likelihood: 12535.321980231818\n",
    "   Iteration: 13, Negative Log-likelihood: 13840.393040802945\n",
    "   Iteration: 14, Negative Log-likelihood: 10832.915703441435\n",
    "   Iteration: 15, Negative Log-likelihood: 12608.384322554397\n",
    "   Iteration: 16, Negative Log-likelihood: 9416.699018656944\n",
    "   Iteration: 17, Negative Log-likelihood: 11230.968871624227\n",
    "   Iteration: 18, Negative Log-likelihood: 8664.16036767125\n",
    "   Iteration: 19, Negative Log-likelihood: 10511.881614018104\n",
    "   Iteration: 20, Negative Log-likelihood: 8005.2145889305175\n",
    "   Iteration: 21, Negative Log-likelihood: 9506.173401452215\n",
    "   Iteration: 22, Negative Log-likelihood: 7872.03070211952\n",
    "   Iteration: 23, Negative Log-likelihood: 9151.001369691996\n",
    "   Iteration: 24, Negative Log-likelihood: 7243.463836711802\n",
    "   Iteration: 25, Negative Log-likelihood: 8598.013450025532\n",
    "   Iteration: 26, Negative Log-likelihood: 6944.612021553246\n",
    "   Iteration: 27, Negative Log-likelihood: 8285.050899480455\n",
    "   Iteration: 28, Negative Log-likelihood: 6617.5511447351055\n",
    "   Iteration: 29, Negative Log-likelihood: 7917.695073969524\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 7917.695073969524\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/AL/CRF_2\"\n",
    "* Elapsed time: 336.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_crf_2 = AL/'dev.crf2.out'\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.test(AL_dev_y, AL_crf_2)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_crf_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Entity  F: 0.9251\n",
    "\n",
    "Sentiment  F: 0.8688"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 4 : SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also explored Structural perceptron algorithm since it is just changing the sum operation into max. although we have gotten comparable results with CRF, this approach is faster than CRF by a small amount of time.\n",
    "\n",
    "we applied CRF as our final submission because the result is stable and will always converge to a stable value. whereas SP may not converge to the small amount of loss every time we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_SP_1=EN_para_FOLDER/'SP_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_SP_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "sp = DM.get_SP(datafile, modelfile, regularization)\n",
    "sp.load_data()\n",
    "sp.train(epoch,lr=)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Entity  F: 0.7614\n",
    "\n",
    "Sentiment  F: 0.6535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_SP_1=AL_para_FOLDER/'SP_1'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_SP_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "sp = DM.get_SP(datafile, modelfile, regularization)\n",
    "sp.load_data()\n",
    "sp.train(epoch,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** SP ********\n",
    "Entity  F: 0.9049\n",
    "\n",
    "Sentiment  F: 0.8970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 5 : SSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we have done an analysis on our dataset, we have found that for some label such as \"S-BAR\" for chinese address parsing, it is predicted with a very low accuracy. therefore, we have thought of adding a \"structual loss\" to that kind of label using Structural SVM algorithm. after adding the loss, the model clearly predict the tag with higher loss better, but it hurts the overall model performance. so we did not apply this model in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_SSVM_1=AL_para_FOLDER/'SSVM_1'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_SSVM_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "ssvm = DM.get_SSVM(datafile, modelfile, regularization)\n",
    "ssvm.load_data()\n",
    "ssvm.train(epoch,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model performance for AL, since SSVM is only created to deal with imbalanced tagging problem in AL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Entity  F: 0.8614\n",
    "\n",
    "Sentiment  F: 0.8535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. found out better and efficient algorithm for implementation of batch training for CRF, probability incorporating stochastic gradient descent.\n",
    "2. implement latent tree CRF mentioned in this paper: https://www.aclweb.org/anthology/N19-1346/ for chinese address parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
