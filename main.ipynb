{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "### Hong Pengfei               \n",
    "### Gao Yunyi \n",
    "### Wu Tianyu\n",
    " -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PGM.GM.HMM.hidden_markov_model import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input data is stored in List[List[str]] where: <br>\n",
    "#### for sentences:  each string is one word, each sublist is one sentence<br>\n",
    "#### for label: each string is one label, each sublist is labels for one sentence in corresponding sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMM.hidden_markov_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('./dataset/')\n",
    "AL = DATA_FOLDER/'AL'\n",
    "AL_train = AL/'train'\n",
    "AL_dev_x = AL/'dev.in'\n",
    "AL_dev_y = AL/'dev.out'\n",
    "AL_out_2 = AL/'dev.p2.out'\n",
    "AL_out_3 = AL/'dev.p3.out'\n",
    "AL_out_4 = AL/'dev.p4.out'\n",
    "AL_crf_1 = AL/'dev.crf1.out'\n",
    "Al_crf_2 = AL/'dev.crf2.out'\n",
    "AL_memm_1 = AL/'dev.memm1.out'\n",
    "AL_sp_1 = AL/'dev.sp1.out'\n",
    "\n",
    "\n",
    "EN = DATA_FOLDER/'EN'\n",
    "EN_train = EN/'train'\n",
    "EN_dev_x = EN/'dev.in'\n",
    "EN_dev_y = EN/'dev.out'\n",
    "EN_out_2 = EN/'dev.p2.out'\n",
    "EN_out_3 = EN/'dev.p3.out'\n",
    "EN_out_4 = EN/'dev.p4.out'\n",
    "EN_crf_1 = EN/'dev.crf1.out'\n",
    "EN_memm_1 = EN/'dev.memm1.out'\n",
    "EN_sp_1 = EN/'dev.sp1.out'\n",
    "\n",
    "CN = DATA_FOLDER/'CN'\n",
    "CN_train = CN/'train'\n",
    "CN_dev_x = CN/'dev.in'\n",
    "CN_dev_y = CN/'dev.out'\n",
    "CN_out_2 = CN/'dev.p2.out'\n",
    "CN_out_3 = CN/'dev.p3.out'\n",
    "\n",
    "SG = DATA_FOLDER/'SG'\n",
    "SG_train = SG/'train'\n",
    "SG_dev_x = SG/'dev.in'\n",
    "SG_dev_y = SG/'dev.out'\n",
    "SG_out_2 = SG/'dev.p2.out'\n",
    "SG_out_3 = SG/'dev.p3.out'\n",
    "\n",
    "PARA_FOLDER = DATA_FOLDER/'parameter'\n",
    "AL_para_FOLDER = PARA_FOLDER/'AL'\n",
    "EN_para_FOLDER = PARA_FOLDER/'EN'\n",
    "\n",
    "\n",
    "EVAL_script = './EvalScript/evalResult.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count is stored in a $|V|\\times|T|$ matrix\n",
    "\n",
    "#### The emission probabilities are obtained by dividing each entry in the counting matrix by the sum of corresponding row\n",
    "\n",
    "#### The prediction algorithm is simple: find the maximum entry's column index and convert it to tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 19484\n",
      "\n",
      "#Correct Entity : 2898\n",
      "Entity  precision: 0.1487\n",
      "Entity  recall: 0.3447\n",
      "Entity  F: 0.2078\n",
      "\n",
      "#Correct Sentiment : 2457\n",
      "Sentiment  precision: 0.1261\n",
      "Sentiment  recall: 0.2922\n",
      "Sentiment  F: 0.1762\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(AL_dev_y, AL_out_2)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 19406\n",
      "\n",
      "#Correct Entity : 9152\n",
      "Entity  precision: 0.4716\n",
      "Entity  recall: 0.6944\n",
      "Entity  F: 0.5617\n",
      "\n",
      "#Correct Sentiment : 7644\n",
      "Sentiment  precision: 0.3939\n",
      "Sentiment  recall: 0.5800\n",
      "Sentiment  F: 0.4692\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(EN_dev_y, EN_out_2)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 18451\n",
      "\n",
      "#Correct Entity : 2632\n",
      "Entity  precision: 0.1426\n",
      "Entity  recall: 0.5801\n",
      "Entity  F: 0.2290\n",
      "\n",
      "#Correct Sentiment : 1239\n",
      "Sentiment  precision: 0.0672\n",
      "Sentiment  recall: 0.2731\n",
      "Sentiment  F: 0.1078\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(SG_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(SG_dev_y, SG_out_2)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 9373\n",
      "\n",
      "#Correct Entity : 765\n",
      "Entity  precision: 0.0816\n",
      "Entity  recall: 0.5176\n",
      "Entity  F: 0.1410\n",
      "\n",
      "#Correct Sentiment : 285\n",
      "Sentiment  precision: 0.0304\n",
      "Sentiment  recall: 0.1928\n",
      "Sentiment  F: 0.0525\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(CN_train)\n",
    "hmm.train_emission()\n",
    "hmm.navie_predict(CN_dev_y, CN_out_2)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <SOS\\> and <EOS\\> are added at the beginning and end of sentence respectively.\n",
    "\n",
    "#### Traverse through each sentence. \n",
    "\n",
    "#### The count is stored in a $(|T|+1)\\times(|T|+1)$ matrix. The rows are initial state and columns are the state trasitted to.\n",
    "\n",
    "#### The probabilities are calculated by dividing each entry with the sum of correspodning row.\n",
    "\n",
    "#### For Viterbi Algorithm, one parent matrix and one score matrix of size $n\\times(|T|+1)$ are implemented\n",
    "\n",
    "#### The optimal path is restored by finding the argmax among $|T|$ tags in previous position\n",
    "\n",
    "#### In our implementation, the code snippet is the same as Part 4 since Part 4 addresses a more general problem.\n",
    "\n",
    "#### To make use of code snippet for Part 4, simply set k=1 for the optimal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 8475\n",
      "\n",
      "#Correct Entity : 6717\n",
      "Entity  precision: 0.7926\n",
      "Entity  recall: 0.7989\n",
      "Entity  F: 0.7957\n",
      "\n",
      "#Correct Sentiment : 6068\n",
      "Sentiment  precision: 0.7160\n",
      "Sentiment  recall: 0.7217\n",
      "Sentiment  F: 0.7188\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(AL_dev_y, AL_out_3, k=1)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 13398\n",
      "\n",
      "#Correct Entity : 11024\n",
      "Entity  precision: 0.8228\n",
      "Entity  recall: 0.8365\n",
      "Entity  F: 0.8296\n",
      "\n",
      "#Correct Sentiment : 10422\n",
      "Sentiment  precision: 0.7779\n",
      "Sentiment  recall: 0.7908\n",
      "Sentiment  F: 0.7843\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(EN_dev_y, EN_out_3, k=1)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 3007\n",
      "\n",
      "#Correct Entity : 1661\n",
      "Entity  precision: 0.5524\n",
      "Entity  recall: 0.3661\n",
      "Entity  F: 0.4403\n",
      "\n",
      "#Correct Sentiment : 1035\n",
      "Sentiment  precision: 0.3442\n",
      "Sentiment  recall: 0.2281\n",
      "Sentiment  F: 0.2744\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(SG_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(SG_dev_y, SG_out_3, k=1)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 713\n",
      "\n",
      "#Correct Entity : 307\n",
      "Entity  precision: 0.4306\n",
      "Entity  recall: 0.2077\n",
      "Entity  F: 0.2802\n",
      "\n",
      "#Correct Sentiment : 210\n",
      "Sentiment  precision: 0.2945\n",
      "Sentiment  recall: 0.1421\n",
      "Sentiment  F: 0.1917\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(CN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(CN_dev_y, CN_out_3, k=1)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Top 7th sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parent 'matrix' and score 'matrix' are implemented as a numpy 3-d array of size $(n+2)\\times(|T|+1)\\times K$.\n",
    "\n",
    "#### The new dimension stores the top K scores and corresponding parent nodes for all positions and tags\n",
    "\n",
    "#### How to backtrack K-th best path:\n",
    "\n",
    "#### Repetition begins: select the parent tags for the first K paths for the <EOS\\> position and identify the parent tag 'T' corresponding to the K-th best path among all $|T|\\times K$ possible paths\n",
    "\n",
    "#### Count the number of apperances of 'T' in the K parent tags for the first K paths.  Call this number ORDER\n",
    "\n",
    "#### Repeat the repetitive steps describe above for one position before, this time select first ORDER instead of K\n",
    "\n",
    "#### Terminate until recovering the entire path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 8583\n",
      "\n",
      "#Correct Entity : 6611\n",
      "Entity  precision: 0.7702\n",
      "Entity  recall: 0.7863\n",
      "Entity  F: 0.7782\n",
      "\n",
      "#Correct Sentiment : 5834\n",
      "Sentiment  precision: 0.6797\n",
      "Sentiment  recall: 0.6939\n",
      "Sentiment  F: 0.6867\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(AL_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(AL_dev_y, AL_out_4, k=7)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_out_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 12777\n",
      "\n",
      "#Correct Entity : 10789\n",
      "Entity  precision: 0.8444\n",
      "Entity  recall: 0.8187\n",
      "Entity  F: 0.8313\n",
      "\n",
      "#Correct Sentiment : 10375\n",
      "Sentiment  precision: 0.8120\n",
      "Sentiment  recall: 0.7872\n",
      "Sentiment  F: 0.7994\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(EN_train)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(EN_dev_y, EN_out_4, k=7)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_out_4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 1 : HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Entity in gold data: 13179\n",
    "#Entity in prediction: 13398\n",
    "\n",
    "#Correct Entity : 11024\n",
    "Entity  precision: 0.8228\n",
    "Entity  recall: 0.8365\n",
    "Entity  F: 0.8296\n",
    "\n",
    "#Correct Sentiment : 10422\n",
    "Sentiment  precision: 0.7779\n",
    "Sentiment  recall: 0.7908\n",
    "Sentiment  F: 0.7843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Entity in gold data: 8408\n",
    "#Entity in prediction: 8475\n",
    "\n",
    "#Correct Entity : 6717\n",
    "Entity  precision: 0.7926\n",
    "Entity  recall: 0.7989\n",
    "Entity  F: 0.7957\n",
    "\n",
    "#Correct Sentiment : 6068\n",
    "Sentiment  precision: 0.7160\n",
    "Sentiment  recall: 0.7217\n",
    "Sentiment  F: 0.7188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 2 : MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_MEMM_1=EN_para_FOLDER/'MEMM_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_MEMM_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "memm = DM.get_MEMM(datafile, modelfile, regularization)\n",
    "memm.load_model(modelfile)\n",
    "memm.load_data()\n",
    "memm.train(epoch,lr=0.0.05,start_epoch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** MEMM *********\n",
    "* Number of labels: 21\n",
    "* Number of features: 801120\n",
    "* Initialized weight of size: 801120\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: 552971.0259626366\n",
    "   Iteration: 1, Negative Log-likelihood: 478521.49781973567\n",
    "   Iteration: 2, Negative Log-likelihood: 453770.10612330795\n",
    "   Iteration: 3, Negative Log-likelihood: 440679.04187276337\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 440679.04187276337\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/EN/MEMM_1\"\n",
    "* Elapsed time: 40.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = DM.get_MEMM(datafile, modelfile, regularization)\n",
    "memm.load_model(modelfile)\n",
    "memm.test(EN_dev_y, EN_memm_1)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_memm_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** MEMM *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/EN/dev.memm1.out\"\n",
    "\n",
    "#Entity in gold data: 13179\n",
    "#Entity in prediction: 12353\n",
    "\n",
    "#Correct Entity : 9999\n",
    "Entity  precision: 0.8094\n",
    "Entity  recall: 0.7587\n",
    "Entity  F: 0.7833\n",
    "\n",
    "#Correct Sentiment : 9318\n",
    "Sentiment  precision: 0.7543\n",
    "Sentiment  recall: 0.7070\n",
    "Sentiment  F: 0.7299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_MEMM_1=AL_para_FOLDER/'MEMM_1'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_MEMM_1\n",
    "regularization = 10\n",
    "epoch = 3\n",
    "memm = DM.get_MEMM(datafile, modelfile, regularization)\n",
    "memm.load_data()\n",
    "memm.train(epoch,lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** MEMM *********\n",
    "* Number of labels: 42\n",
    "* Number of features: 443993\n",
    "* Initialized weight of size: 443993\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 1, Negative Log-likelihood: 653901.5620625856\n",
    "   Iteration: 2, Negative Log-likelihood: 144292.62151513598\n",
    "   Iteration: 3, Negative Log-likelihood: 135078.24683414222\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 135078.24683414222\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/AL/MEMM_1\"\n",
    "* Elapsed time: 51.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = memm.get_SP(datafile, modelfile, regularization)\n",
    "memm.load_model(modelfile)\n",
    "memm.test(AL_dev_y, AL_memm_1)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_memm_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ******** SP *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/AL/dev.memm1.out\"\n",
    "\n",
    "#Entity in gold data: 8408\n",
    "#Entity in prediction: 11282\n",
    "\n",
    "#Correct Entity : 5951\n",
    "Entity  precision: 0.5275\n",
    "Entity  recall: 0.7078\n",
    "Entity  F: 0.6045\n",
    "\n",
    "#Correct Sentiment : 5467\n",
    "Sentiment  precision: 0.4846\n",
    "Sentiment  recall: 0.6502\n",
    "Sentiment  F: 0.5553"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 3 : CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_CRF_1=EN_para_FOLDER/'CRF_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_CRF_1\n",
    "regularization = 10\n",
    "epoch = 10\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.load_data()\n",
    "crf.train(epoch,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** CRF *********\n",
    "* Number of labels: 21\n",
    "* Number of features: 801120\n",
    "* Initialized weight of size: 801120\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: 552971.0259626366\n",
    "   Iteration: 1, Negative Log-likelihood: 302239.3705810338\n",
    "   Iteration: 2, Negative Log-likelihood: 329765.2072340142\n",
    "   Iteration: 3, Negative Log-likelihood: 183565.70088908664\n",
    "   Iteration: 4, Negative Log-likelihood: 176268.4506836803\n",
    "   Iteration: 5, Negative Log-likelihood: 134368.8650630805\n",
    "   Iteration: 6, Negative Log-likelihood: 128499.3068339848\n",
    "   Iteration: 7, Negative Log-likelihood: 109340.86827536151\n",
    "   Iteration: 8, Negative Log-likelihood: 103687.0264011708\n",
    "   Iteration: 9, Negative Log-likelihood: 94182.9171344954\n",
    "   Iteration: 10, Negative Log-likelihood: 88362.10235672901\n",
    "   Iteration: 11, Negative Log-likelihood: 82989.46484183696\n",
    "   Iteration: 12, Negative Log-likelihood: 78647.88325730735\n",
    "   Iteration: 13, Negative Log-likelihood: 74059.73784250424\n",
    "   Iteration: 14, Negative Log-likelihood: 69997.25080941238\n",
    "   Iteration: 15, Negative Log-likelihood: 66194.08169550923\n",
    "   Iteration: 16, Negative Log-likelihood: 62948.85094579896\n",
    "   Iteration: 17, Negative Log-likelihood: 59921.31276105164\n",
    "   Iteration: 18, Negative Log-likelihood: 57162.635653787314\n",
    "   Iteration: 19, Negative Log-likelihood: 54552.22178416393\n",
    "   Iteration: 20, Negative Log-likelihood: 52135.11081752372\n",
    "   Iteration: 21, Negative Log-likelihood: 49880.53280020891\n",
    "   Iteration: 22, Negative Log-likelihood: 47795.5716922185\n",
    "   Iteration: 23, Negative Log-likelihood: 45830.34424435205\n",
    "   Iteration: 24, Negative Log-likelihood: 44025.43582541786\n",
    "   Iteration: 25, Negative Log-likelihood: 42271.56740645353\n",
    "   Iteration: 26, Negative Log-likelihood: 40644.70984352812\n",
    "   Iteration: 27, Negative Log-likelihood: 39092.00620064286\n",
    "   Iteration: 28, Negative Log-likelihood: 37642.22401288922\n",
    "   Iteration: 29, Negative Log-likelihood: 36268.548014728694\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 36268.548014728694\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/EN/CRF_1\"\n",
    "* Elapsed time: 229.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.test(EN_dev_y,EN_crf_1)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_crf_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** CRF *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/EN/dev.crf1.out\"\n",
    "\n",
    "#Entity in gold data: 13179\n",
    "#Entity in prediction: 12980\n",
    "\n",
    "#Correct Entity : 11584\n",
    "Entity  precision: 0.8924\n",
    "Entity  recall: 0.8790\n",
    "Entity  F: 0.8857\n",
    "\n",
    "#Correct Sentiment : 11208\n",
    "Sentiment  precision: 0.8635\n",
    "Sentiment  recall: 0.8504\n",
    "Sentiment  F: 0.8569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_CRF_2=AL_para_FOLDER/'CRF_2'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_CRF_2\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_data()\n",
    "crf.train(epoch,lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "******** CRF *********\n",
    "* Number of labels: 42\n",
    "* Number of features: 443993\n",
    "* Initialized weight of size: 443993\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: 653901.5620625856\n",
    "   Iteration: 1, Negative Log-likelihood: 71151.57979157224\n",
    "   Iteration: 2, Negative Log-likelihood: 134826.53628937004\n",
    "   Iteration: 3, Negative Log-likelihood: 35996.291548169036\n",
    "   Iteration: 4, Negative Log-likelihood: 50068.4311422095\n",
    "   Iteration: 5, Negative Log-likelihood: 35184.22091465483\n",
    "   Iteration: 6, Negative Log-likelihood: 29531.13816272047\n",
    "   Iteration: 7, Negative Log-likelihood: 25449.639322340125\n",
    "   Iteration: 8, Negative Log-likelihood: 20163.900435345706\n",
    "   Iteration: 9, Negative Log-likelihood: 19568.620540475848\n",
    "   Iteration: 10, Negative Log-likelihood: 15701.300994498764\n",
    "   Iteration: 11, Negative Log-likelihood: 16576.333405049903\n",
    "   Iteration: 12, Negative Log-likelihood: 12535.321980231818\n",
    "   Iteration: 13, Negative Log-likelihood: 13840.393040802945\n",
    "   Iteration: 14, Negative Log-likelihood: 10832.915703441435\n",
    "   Iteration: 15, Negative Log-likelihood: 12608.384322554397\n",
    "   Iteration: 16, Negative Log-likelihood: 9416.699018656944\n",
    "   Iteration: 17, Negative Log-likelihood: 11230.968871624227\n",
    "   Iteration: 18, Negative Log-likelihood: 8664.16036767125\n",
    "   Iteration: 19, Negative Log-likelihood: 10511.881614018104\n",
    "   Iteration: 20, Negative Log-likelihood: 8005.2145889305175\n",
    "   Iteration: 21, Negative Log-likelihood: 9506.173401452215\n",
    "   Iteration: 22, Negative Log-likelihood: 7872.03070211952\n",
    "   Iteration: 23, Negative Log-likelihood: 9151.001369691996\n",
    "   Iteration: 24, Negative Log-likelihood: 7243.463836711802\n",
    "   Iteration: 25, Negative Log-likelihood: 8598.013450025532\n",
    "   Iteration: 26, Negative Log-likelihood: 6944.612021553246\n",
    "   Iteration: 27, Negative Log-likelihood: 8285.050899480455\n",
    "   Iteration: 28, Negative Log-likelihood: 6617.5511447351055\n",
    "   Iteration: 29, Negative Log-likelihood: 7917.695073969524\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 7917.695073969524\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/AL/CRF_2\"\n",
    "* Elapsed time: 336.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_crf_2 = AL/'dev.crf2.out'\n",
    "crf = DM.get_CRF(datafile, modelfile, regularization)\n",
    "crf.load_model(modelfile)\n",
    "crf.test(AL_dev_y, AL_crf_2)\n",
    "! python3 {EVAL_script} {AL_dev_y} {AL_crf_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** CRF *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/AL/dev.crf2.out\"\n",
    "\n",
    "#Entity in gold data: 8408\n",
    "#Entity in prediction: 8456\n",
    "\n",
    "#Correct Entity : 7708\n",
    "Entity  precision: 0.9115\n",
    "Entity  recall: 0.9167\n",
    "Entity  F: 0.9141\n",
    "\n",
    "#Correct Sentiment : 7001\n",
    "Sentiment  precision: 0.8279\n",
    "Sentiment  recall: 0.8327\n",
    "Sentiment  F: 0.8303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 4 : SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_para_SP_1=EN_para_FOLDER/'SP_1'\n",
    "datafile = EN_train\n",
    "modelfile = EN_para_SP_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "sp = DM.get_SP(datafile, modelfile, regularization)\n",
    "sp.load_data()\n",
    "sp.train(epoch,lr=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** SP *********\n",
    "* Number of labels: 21\n",
    "* Number of features: 801120\n",
    "* Initialized weight of size: 801120\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: -0.0\n",
    "   Iteration: 1, Negative Log-likelihood: 8021616.55\n",
    "   Iteration: 2, Negative Log-likelihood: 1695553.3713108923\n",
    "   Iteration: 3, Negative Log-likelihood: 280246.7473569624\n",
    "   Iteration: 4, Negative Log-likelihood: 458493.96621427033\n",
    "   Iteration: 5, Negative Log-likelihood: 212221.5087348179\n",
    "   Iteration: 6, Negative Log-likelihood: 204733.746371097\n",
    "   Iteration: 7, Negative Log-likelihood: 144506.77266386698\n",
    "   Iteration: 8, Negative Log-likelihood: 141086.6347840046\n",
    "   Iteration: 9, Negative Log-likelihood: 118078.45694097657\n",
    "   Iteration: 10, Negative Log-likelihood: 108236.7570168846\n",
    "   Iteration: 11, Negative Log-likelihood: 96730.2765246267\n",
    "   Iteration: 12, Negative Log-likelihood: 89602.11118785565\n",
    "   Iteration: 13, Negative Log-likelihood: 83927.10712654018\n",
    "   Iteration: 14, Negative Log-likelihood: 79068.11244089795\n",
    "   Iteration: 15, Negative Log-likelihood: 74900.93956908103\n",
    "   Iteration: 16, Negative Log-likelihood: 70995.15321706317\n",
    "   Iteration: 17, Negative Log-likelihood: 67347.26584531067\n",
    "   Iteration: 18, Negative Log-likelihood: 64006.56526933432\n",
    "   Iteration: 19, Negative Log-likelihood: 61401.66786022362\n",
    "   Iteration: 20, Negative Log-likelihood: 58527.83105116691\n",
    "   Iteration: 21, Negative Log-likelihood: 56021.737535479435\n",
    "   Iteration: 22, Negative Log-likelihood: 53737.29014376587\n",
    "   Iteration: 23, Negative Log-likelihood: 51801.0917145915\n",
    "   Iteration: 24, Negative Log-likelihood: 49772.88548264665\n",
    "   Iteration: 25, Negative Log-likelihood: 48033.62967644008\n",
    "   Iteration: 26, Negative Log-likelihood: 46393.10228000874\n",
    "   Iteration: 27, Negative Log-likelihood: 44930.39353390147\n",
    "   Iteration: 28, Negative Log-likelihood: 43215.573229966\n",
    "   Iteration: 29, Negative Log-likelihood: 41704.75767130613\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 41704.75767130613\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/EN/SP_1\"\n",
    "* Elapsed time: 185.0 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = DM.get_SP(datafile, modelfile, regularization)\n",
    "sp.load_model(modelfile)\n",
    "sp.test(EN_dev_y, EN_sp_1)\n",
    "! python3 {EVAL_script} {EN_dev_y} {EN_sp_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** SP *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/EN/dev.sp1.out\"\n",
    "\n",
    "#Entity in gold data: 13179\n",
    "#Entity in prediction: 15266\n",
    "\n",
    "#Correct Entity : 10829\n",
    "Entity  precision: 0.7094\n",
    "Entity  recall: 0.8217\n",
    "Entity  F: 0.7614\n",
    "\n",
    "#Correct Sentiment : 9294\n",
    "Sentiment  precision: 0.6088\n",
    "Sentiment  recall: 0.7052\n",
    "Sentiment  F: 0.6535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_para_SP_1=AL_para_FOLDER/'SP_1'\n",
    "datafile = AL_train\n",
    "modelfile = AL_para_SP_1\n",
    "regularization = 10\n",
    "epoch = 30\n",
    "sp = DM.get_SP(datafile, modelfile, regularization)\n",
    "sp.load_data()\n",
    "sp.train(epoch,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "******** SP *********\n",
    "* Number of labels: 42\n",
    "* Number of features: 443993\n",
    "* Initialized weight of size: 443993\n",
    " ******** Start Training *********\n",
    "* Squared sigma: 10\n",
    "* Start Gradient Descend\n",
    "   ========================\n",
    "   iter(sit): Negative log-likelihood\n",
    "   ------------------------\n",
    "   Iteration: 0, Negative Log-likelihood: -0.0\n",
    "   Iteration: 1, Negative Log-likelihood: 9862054.95\n",
    "   Iteration: 2, Negative Log-likelihood: 2798728.345140766\n",
    "   Iteration: 3, Negative Log-likelihood: 471298.74783530453\n",
    "   Iteration: 4, Negative Log-likelihood: 1899117.221298697\n",
    "   Iteration: 5, Negative Log-likelihood: 471699.83451168425\n",
    "   Iteration: 6, Negative Log-likelihood: 646951.0982014483\n",
    "   Iteration: 7, Negative Log-likelihood: 419341.3805890942\n",
    "   Iteration: 8, Negative Log-likelihood: 545010.4467075918\n",
    "   Iteration: 9, Negative Log-likelihood: 384834.938699189\n",
    "   Iteration: 10, Negative Log-likelihood: 402615.3786619578\n",
    "   Iteration: 11, Negative Log-likelihood: 290481.2921698009\n",
    "   Iteration: 12, Negative Log-likelihood: 328533.9478094559\n",
    "   Iteration: 13, Negative Log-likelihood: 239193.77526421286\n",
    "   Iteration: 14, Negative Log-likelihood: 268626.89726078254\n",
    "   Iteration: 15, Negative Log-likelihood: 205015.3630225027\n",
    "   Iteration: 16, Negative Log-likelihood: 225350.2496771266\n",
    "   Iteration: 17, Negative Log-likelihood: 178706.37468427938\n",
    "   Iteration: 18, Negative Log-likelihood: 202795.99820130778\n",
    "   Iteration: 19, Negative Log-likelihood: 158682.5567637166\n",
    "   Iteration: 20, Negative Log-likelihood: 179673.45557705066\n",
    "   Iteration: 21, Negative Log-likelihood: 145630.46754843183\n",
    "   Iteration: 22, Negative Log-likelihood: 168170.87368902907\n",
    "   Iteration: 23, Negative Log-likelihood: 135890.65079782973\n",
    "   Iteration: 24, Negative Log-likelihood: 155444.4854795329\n",
    "   Iteration: 25, Negative Log-likelihood: 128338.28495657464\n",
    "   Iteration: 26, Negative Log-likelihood: 140699.51277462294\n",
    "   Iteration: 27, Negative Log-likelihood: 118158.79110216882\n",
    "   Iteration: 28, Negative Log-likelihood: 130673.91991640948\n",
    "   Iteration: 29, Negative Log-likelihood: 112889.3196615286\n",
    "   ========================\n",
    "   (iter: iteration, sit: sub iteration)\n",
    "* Likelihood: 112889.3196615286\n",
    " ******** Finished Training *********\n",
    "* Trained CRF Model has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/parameter/AL/SP_1\"\n",
    "* Elapsed time: 308.0 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ******** SP *********\n",
    "CRF model loaded\n",
    "* Test output has been saved at \"/home/pengfei/sutd-machine-learning-project/dataset/AL/dev.sp1.out\"\n",
    "\n",
    "#Entity in gold data: 8408\n",
    "#Entity in prediction: 8965\n",
    "\n",
    "#Correct Entity : 6905\n",
    "Entity  precision: 0.7702\n",
    "Entity  recall: 0.8212\n",
    "Entity  F: 0.7949\n",
    "\n",
    "#Correct Sentiment : 5968\n",
    "Sentiment  precision: 0.6657\n",
    "Sentiment  recall: 0.7098\n",
    "Sentiment  F: 0.6870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 5 : SSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "------------------------------------------------------\n",
    "## Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of HMM:\n",
    "1. learns distribution between p(x, y), performs generally better when there is an small dataset\n",
    "\n",
    "### Disadvantage of HMM:\n",
    "\n",
    "2. HMM is a generative model modelling the distributon of x,y at the same time. When the distribution of x for training dataset is different from the distribution of x for testing dataset (which is very likely), the performance on testing dataset would be very poor.\n",
    "\n",
    "3. Features of input x is ignored in the model. In NLP tasks, the variety of features in languages is discarded and ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy Markov Model (MEMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advatange of MEMM:\n",
    "\n",
    "1. Discriminative model, solving the issue mentioned in disadvantage of HMM\n",
    "http://localhost:8888/notebooks/main.ipynb#\n",
    "2. Flexible for different feature functions, mining language features\n",
    "\n",
    "### Disadvantage of MEMM:\n",
    "\n",
    "Labelling bias: MEMM sometimes cannot capture the general trend of state transitions because it adopts local variance normalization. The local optimal solution is achieved but it might not be the globally optimal one. The model is limited by this issue, especially when the numbers of convertible states vary significantly among states. Too many convertible states diluted the transition probability, resulting in unequal optimal solution and global solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, we move on from MEMM to CRF, which addresses the labelling bias problem perfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields (CRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of CRF:\n",
    "\n",
    "1. Compared to HMM: There is no Markov assumptions, which is sometimes unreasonable.\n",
    "\n",
    "2. Compared to MEMM: Global variance normalization is adopted. Global trend is captured. Labelling bias problem is addressed.\n",
    "\n",
    "3. Feature functions are flexible, same as MEMM\n",
    "\n",
    "### Disadvantage of CRF:\n",
    "\n",
    "1. Computationally complicated. It takes our model hours and hours to train. This also results in low flexibility when new training data is availble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Perceptron( SP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
