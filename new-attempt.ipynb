{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('./dataset/')\n",
    "AL = DATA_FOLDER/'AL'\n",
    "AL_train = AL/'train'\n",
    "AL_dev_x = AL/'dev.in'\n",
    "AL_dev_y = AL/'dev.out'\n",
    "AL_out_2 = AL/'dev.p2.out'\n",
    "AL_out_3 = AL/'dev.p3.out'\n",
    "AL_out_4 = AL/'dev.p4.out'\n",
    "\n",
    "EN = DATA_FOLDER/'EN'\n",
    "EN_train = EN/'train'\n",
    "EN_dev_x = EN/'dev.in'\n",
    "EN_dev_y = EN/'dev.out'\n",
    "EN_out_2 = EN/'dev.p2.out'\n",
    "EN_out_3 = EN/'dev.p3.out'\n",
    "EN_out_4 = EN/'dev.p4.out'\n",
    "\n",
    "CN = DATA_FOLDER/'CN'\n",
    "CN_train = CN/'train'\n",
    "CN_dev_x = CN/'dev.in'\n",
    "CN_dev_y = CN/'dev.out'\n",
    "CN_out_2 = CN/'dev.p2.out'\n",
    "CN_out_3 = CN/'dev.p3.out'\n",
    "\n",
    "SG = DATA_FOLDER/'SG'\n",
    "SG_train = SG/'train'\n",
    "SG_dev_x = SG/'dev.in'\n",
    "SG_dev_y = SG/'dev.out'\n",
    "SG_out_2 = SG/'dev.p2.out'\n",
    "SG_out_3 = SG/'dev.p3.out'\n",
    "\n",
    "\n",
    "EVAL_script = './EvalScript/evalResult.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    x, y = [], []\n",
    "    temp_x, temp_y = [], []\n",
    "    for l in lines:\n",
    "        if len(l) == 1:\n",
    "            assert(len(temp_x) == len(temp_y))\n",
    "            x.append(temp_x)\n",
    "            y.append(temp_y)\n",
    "            temp_x, temp_y = [], []\n",
    "            continue\n",
    "        xx, yy = l.split()\n",
    "        temp_x.append(xx)\n",
    "        temp_y.append(yy)\n",
    "    if len(temp_x) != 0:\n",
    "        x.append(temp_x)\n",
    "        y.append(temp_y)\n",
    "    assert(len(x) == len(y))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def get_test_data(filename, word2index):\n",
    "    \"\"\"Return:\n",
    "                x: nested list of string\n",
    "                x_int: nested list of integer\"\"\"\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    x = []\n",
    "    temp_x = []\n",
    "    for l in lines:\n",
    "        if len(l.strip()) == 0:\n",
    "            x.append(temp_x)\n",
    "            temp_x = []\n",
    "            continue\n",
    "        xx = l.split()\n",
    "        temp_x.append(xx[0])\n",
    "    if len(temp_x) != 0:\n",
    "        x.append(temp_x)\n",
    "    x_int = [[word2index[oo] for oo in o] for o in x ]\n",
    "    return x, x_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = get_train_data(AL_train)\n",
    "vocab = list(set([oo for o in words for oo in o]))\n",
    "tags = list(set([oo for o in labels for oo in o])) + ['SOS', 'EOS']\n",
    "word2index = {o:i for i,o in enumerate(vocab)}\n",
    "index2word = {i:o for i,o in enumerate(vocab)}\n",
    "tag2index = {o:i for i,o in enumerate(tags)}\n",
    "x = [[word2index[oo] for oo in o] for o in words]\n",
    "y = [[tag2index[oo] for oo in o] for o in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1 Emission features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission(x, y, vocab, tags):\n",
    "    emission = np.zeros((len(vocab), len(tags)))\n",
    "    flat_y = [oo for o in y for oo in o]\n",
    "    flat_x = [oo for o in x for oo in o]\n",
    "    for xx, yy in zip(flat_x,flat_y):\n",
    "        emission[xx, yy] += 1\n",
    "    \n",
    "    y_count = np.zeros(len(tags))\n",
    "    for yy in flat_y:\n",
    "        y_count[yy] += 1\n",
    "    emission = emission/ y_count[None, :]\n",
    "    np.nan_to_num(emission, 0)\n",
    "    return emission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Adding smoothing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "vocab_count = Counter([oo for o in words for oo in o])\n",
    "vocab = [o for o, v in dict(vocab_count).items() if v>=3] + ['#UNK#']\n",
    "word2index = defaultdict(int)\n",
    "for i,o in enumerate(vocab): word2index[o] = i+1\n",
    "x = [ [word2index[oo] for oo in o] for o in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(x, emission_matrix):\n",
    "    \"\"\"emission matrix: (vocab_size, tag_size)\n",
    "    x: converted to integer arrays\"\"\"\n",
    "    return emission_matrix[x].argmax(axis=1)\n",
    "\n",
    "def batch_decoding(output_filename, dev_x_filename, word2index, emission_matrix, tags):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        words, dev_x = get_test_data(dev_x_filename, word2index)\n",
    "        for ws,o in zip(words, dev_x):\n",
    "            path = decoding(o, emission_matrix)\n",
    "            for w, p in zip(ws, path):\n",
    "                f.write(w + ' ' + tags[p] + '\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab everythings together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_file, output_filename, dev_x_filename):\n",
    "    # read data\n",
    "    words, labels = get_train_data(train_file)\n",
    "    # create vocab\n",
    "    tags = list(set([oo for o in labels for oo in o])) + ['SOS', 'EOS']\n",
    "    tag2index = {o:i for i,o in enumerate(tags)}\n",
    "    vocab_count = Counter([oo for o in words for oo in o])\n",
    "    vocab = [o for o, v in dict(vocab_count).items() if v>=3] + ['#UNK#']\n",
    "    word2index = defaultdict(int)\n",
    "    for i,o in enumerate(vocab): word2index[o] = i+1\n",
    "    # text to int\n",
    "    x = [[word2index[oo] for oo in o] for o in words]\n",
    "    y = [[tag2index[oo] for oo in o] for o in labels]\n",
    "    # training\n",
    "    emission_matrix = emission(x, y, vocab, tags)\n",
    "    # decoding\n",
    "    batch_decoding(output_filename, dev_x_filename, word2index, emission_matrix, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 19484\r\n",
      "\r\n",
      "#Correct Entity : 2898\r\n",
      "Entity  precision: 0.1487\r\n",
      "Entity  recall: 0.3447\r\n",
      "Entity  F: 0.2078\r\n",
      "\r\n",
      "#Correct Sentiment : 2457\r\n",
      "Sentiment  precision: 0.1261\r\n",
      "Sentiment  recall: 0.2922\r\n",
      "Sentiment  F: 0.1762\r\n"
     ]
    }
   ],
   "source": [
    "main(AL_train, AL_out_2, AL_dev_x)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4537\r\n",
      "#Entity in prediction: 18451\r\n",
      "\r\n",
      "#Correct Entity : 2632\r\n",
      "Entity  precision: 0.1426\r\n",
      "Entity  recall: 0.5801\r\n",
      "Entity  F: 0.2290\r\n",
      "\r\n",
      "#Correct Sentiment : 1239\r\n",
      "Sentiment  precision: 0.0672\r\n",
      "Sentiment  recall: 0.2731\r\n",
      "Sentiment  F: 0.1078\r\n"
     ]
    }
   ],
   "source": [
    "main(SG_train, SG_out_2, SG_dev_x)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 19406\r\n",
      "\r\n",
      "#Correct Entity : 9152\r\n",
      "Entity  precision: 0.4716\r\n",
      "Entity  recall: 0.6944\r\n",
      "Entity  F: 0.5617\r\n",
      "\r\n",
      "#Correct Sentiment : 7644\r\n",
      "Sentiment  precision: 0.3939\r\n",
      "Sentiment  recall: 0.5800\r\n",
      "Sentiment  F: 0.4692\r\n"
     ]
    }
   ],
   "source": [
    "main(EN_train, EN_out_2, EN_dev_x)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 1478\r\n",
      "#Entity in prediction: 9373\r\n",
      "\r\n",
      "#Correct Entity : 765\r\n",
      "Entity  precision: 0.0816\r\n",
      "Entity  recall: 0.5176\r\n",
      "Entity  F: 0.1410\r\n",
      "\r\n",
      "#Correct Sentiment : 285\r\n",
      "Sentiment  precision: 0.0304\r\n",
      "Sentiment  recall: 0.1928\r\n",
      "Sentiment  F: 0.0525\r\n"
     ]
    }
   ],
   "source": [
    "main(CN_train, CN_out_2, CN_dev_x)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_matrix shape (transition_from, transition_to):  (43, 43)\n"
     ]
    }
   ],
   "source": [
    "def transition(y, tags, tag2index):\n",
    "    \"\"\"\n",
    "    tags: included 'SOS' and 'EOS'\n",
    "    transition from: (v1, v2, ..., 'SOS')\n",
    "    transition to:   (v1, v2, ..., 'EOS')\n",
    "    rows are transition_from\n",
    "    cols are transition_to\n",
    "    \"\"\"\n",
    "    transition = np.zeros((len(tags)-1, len(tags)-1))\n",
    "    for yy in y: \n",
    "        transition[-1, yy[0]] += 1 # START transition\n",
    "        for i in range(len(yy)-1): # tags transition from position 0 to len(yy)-2\n",
    "            transition[yy[i], yy[i+1]] += 1\n",
    "        transition[yy[-1], -1] += 1 # STOP transition\n",
    "    transition = transition/np.sum(transition, axis=1)\n",
    "#     np.nan_to_num(transition, 0)\n",
    "    return transition\n",
    "\n",
    "transition_matrix = transition(y, tags, tag2index)\n",
    "print(\"transition_matrix shape (transition_from, transition_to): \", transition_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viterbi decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, transition_matrix, emission_matrix, tags):\n",
    "    \"\"\"transition_matrix: before log\n",
    "    x: [1, 2, 4, 19, ...]\n",
    "    transition: after log\n",
    "    return: \n",
    "            path: (len(x), )\n",
    "            log(max_score)\n",
    "    \"\"\"\n",
    "\n",
    "    score = np.zeros( (len(x)+2, len(tags)-2) )\n",
    "    argmax = np.zeros( (len(x)+2, len(tags)-2), dtype=np.int)\n",
    "    transition, emission = np.log(transition_matrix), np.log(emission_matrix)\n",
    "    score[1, :] = transition[-1, :-1] + emission[x[0], :-2] # initialization at j=1\n",
    "    for j in range(2, len(x)+1): \n",
    "        for t in range(len(tags)-2):\n",
    "            pi = score[j-1, :]  # (num_of_tags-2,)\n",
    "            a = transition[:-1, t] # (num_of_tags-2,)\n",
    "            b = emission[x[j-1], t] # (1,)\n",
    "            top1 = (pi + a).argsort()[-1]\n",
    "            argmax[j, t] = top1\n",
    "            score[j, t] = (pi + a)[top1] + b\n",
    "    # j=n+1 step\n",
    "    pi = score[len(x)]\n",
    "    a = transition[:-1, -1]\n",
    "    argmax_stop = int( (pi+a).argsort()[-1] )\n",
    "    max_stop = (pi+a)[argmax_stop]\n",
    "    argmax = argmax[2:-1]\n",
    "    # decoding\n",
    "    path = [argmax_stop]\n",
    "    temp_index = argmax_stop\n",
    "    for i in range(len(argmax)-1, -1, -1):\n",
    "        temp_index = argmax[i, temp_index]\n",
    "        path.append(temp_index)\n",
    "    return path[::-1], max_stop\n",
    "\n",
    "\n",
    "def viterbi_decode_batch(x, transition_matrix, emission_matrix,\n",
    "                         output_filename, dev_x_filename, word2index, tags):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        words, dev_x = get_test_data(dev_x_filename, word2index)\n",
    "        for i, (ws,o) in enumerate(zip(words, dev_x)):\n",
    "            path, log_max_score = viterbi(o, transition_matrix, emission_matrix, tags)\n",
    "            for w, p in zip(ws, path):\n",
    "                f.write(w + ' ' + tags[p] + '\\n')\n",
    "            f.write('\\n')\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3(train_file, output_filename, dev_x_filename):\n",
    "    # read data\n",
    "    words, labels = get_train_data(train_file)\n",
    "    # create vocab\n",
    "    tags = list(set([oo for o in labels for oo in o])) + ['SOS', 'EOS']\n",
    "    tag2index = {o:i for i,o in enumerate(tags)}\n",
    "    vocab_count = Counter([oo for o in words for oo in o])\n",
    "    vocab = [o for o, v in dict(vocab_count).items() if v>=3] + ['#UNK#']\n",
    "    word2index = defaultdict(int)\n",
    "    for i,o in enumerate(vocab): word2index[o] = i+1\n",
    "    # text to int\n",
    "    x = [[word2index[oo] for oo in o] for o in words]\n",
    "    y = [[tag2index[oo] for oo in o] for o in labels]\n",
    "    # training emission\n",
    "    emission_matrix = emission(x, y, vocab, tags)\n",
    "    # emission_matrix += 1e-5 Adding this smoothing will increase performance\n",
    "    print(\"emission_matrix shape:\", emission_matrix.shape)\n",
    "    # training transition\n",
    "    transition_matrix = transition(y, tags, tag2index)\n",
    "    print(\"transition_matrix shape\", transition_matrix.shape)\n",
    "    # decoding\n",
    "    viterbi_decode_batch(x, transition_matrix, emission_matrix,\n",
    "                         output_filename, dev_x_filename, word2index, tags)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (2698, 44)\n",
      "transition_matrix shape (43, 43)\n",
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 8520\n",
      "\n",
      "#Correct Entity : 6731\n",
      "Entity  precision: 0.7900\n",
      "Entity  recall: 0.8005\n",
      "Entity  F: 0.7953\n",
      "\n",
      "#Correct Sentiment : 6077\n",
      "Sentiment  precision: 0.7133\n",
      "Sentiment  recall: 0.7228\n",
      "Sentiment  F: 0.7180\n"
     ]
    }
   ],
   "source": [
    "q3(AL_train, AL_out_3, AL_dev_x)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (6187, 23)\n",
      "transition_matrix shape (22, 22)\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 12724\n",
      "\n",
      "#Correct Entity : 10785\n",
      "Entity  precision: 0.8476\n",
      "Entity  recall: 0.8183\n",
      "Entity  F: 0.8327\n",
      "\n",
      "#Correct Sentiment : 10370\n",
      "Sentiment  precision: 0.8150\n",
      "Sentiment  recall: 0.7869\n",
      "Sentiment  F: 0.8007\n"
     ]
    }
   ],
   "source": [
    "q3(EN_train, EN_out_3, EN_dev_x)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (10733, 9)\n",
      "transition_matrix shape (8, 8)\n",
      "\n",
      "#Entity in gold data: 4537\n",
      "#Entity in prediction: 3036\n",
      "\n",
      "#Correct Entity : 1662\n",
      "Entity  precision: 0.5474\n",
      "Entity  recall: 0.3663\n",
      "Entity  F: 0.4389\n",
      "\n",
      "#Correct Sentiment : 1035\n",
      "Sentiment  precision: 0.3409\n",
      "Sentiment  recall: 0.2281\n",
      "Sentiment  F: 0.2733\n"
     ]
    }
   ],
   "source": [
    "q3(SG_train, SG_out_3, SG_dev_x)\n",
    "! python {EVAL_script} {SG_dev_y} {SG_out_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (7364, 9)\n",
      "transition_matrix shape (8, 8)\n",
      "\n",
      "#Entity in gold data: 1478\n",
      "#Entity in prediction: 769\n",
      "\n",
      "#Correct Entity : 309\n",
      "Entity  precision: 0.4018\n",
      "Entity  recall: 0.2091\n",
      "Entity  F: 0.2750\n",
      "\n",
      "#Correct Sentiment : 210\n",
      "Sentiment  precision: 0.2731\n",
      "Sentiment  recall: 0.1421\n",
      "Sentiment  F: 0.1869\n"
     ]
    }
   ],
   "source": [
    "q3(CN_train, CN_out_3, CN_dev_x)\n",
    "! python {EVAL_script} {CN_dev_y} {CN_out_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Top 7th sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_top_k(x, transition_matrix, emission_matrix, tags, k=7):\n",
    "    \"\"\"transition_matrix: before log\n",
    "    x: [1, 2, 4, 19, ...]\n",
    "    transition: after log\n",
    "    time complexity: O(knt^2)\n",
    "    return: \n",
    "            path: (len(x), )\n",
    "            log(max_score)\n",
    "    \"\"\"\n",
    "\n",
    "    score = np.zeros( (len(x)+2, len(tags)-2, 7) )\n",
    "    argmax = np.zeros( (len(x)+2, len(tags)-2, 7), dtype=np.int)\n",
    "    transition, emission = np.log(transition_matrix), np.log(emission_matrix)\n",
    "    # initialization at j=1\n",
    "    score[1, :] = (transition[-1, :-1] + emission[x[0], :-2])[:, None] \n",
    "    for j in range(2, len(x)+1): \n",
    "        for t in range(len(tags)-2):\n",
    "            pi = score[j-1, :]  # (num_of_tags-2, 7)\n",
    "            a = transition[:-1, t] # (num_of_tags-2,)\n",
    "            b = emission[x[j-1], t] # (1,)\n",
    "            previous_all_scores = (pi + a[:,None]).flatten()\n",
    "            topk = previous_all_scores.argsort()[-k:][::-1] # big to small\n",
    "            argmax[j, t] = topk // 7\n",
    "            score[j, t] = previous_all_scores[topk] + b\n",
    "            \n",
    "    # j=n+1 step\n",
    "    pi = score[len(x)] # (num_of_tags-2, 7)\n",
    "    a = transition[:-1, -1]\n",
    "    argmax_stop_k = (pi + a[:,None]).flatten().argsort()[-k:][::-1][-1]//7 # big to small\n",
    "    max_stop = np.max(pi+a[:,None])\n",
    "    argmax = argmax[2:-1] # (len(x)-1, num_of_tags-2, 7)\n",
    "    \n",
    "    # decoding\n",
    "    path = [argmax_stop_k]\n",
    "    temp_index = argmax_stop_k\n",
    "    for i in range(len(argmax)-1, -1, -1):\n",
    "        temp_index = argmax[i, temp_index, 0]\n",
    "        path.append(temp_index)\n",
    "    return path[::-1], max_stop\n",
    "\n",
    "def viterbi_decode_batch(x, transition_matrix, emission_matrix,\n",
    "                         output_filename, dev_x_filename, word2index, tags):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        words, dev_x = get_test_data(dev_x_filename, word2index)\n",
    "        for i, (ws,o) in enumerate(zip(words, dev_x)):\n",
    "            path, log_max_score = viterbi_top_k(o, transition_matrix, emission_matrix, tags)\n",
    "            for w, p in zip(ws, path):\n",
    "                f.write(w + ' ' + tags[p] + '\\n')\n",
    "            f.write('\\n')\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4(train_file, output_filename, dev_x_filename):\n",
    "    # read data\n",
    "    words, labels = get_train_data(train_file)\n",
    "    # create vocab\n",
    "    tags = list(set([oo for o in labels for oo in o])) + ['SOS', 'EOS']\n",
    "    tag2index = {o:i for i,o in enumerate(tags)}\n",
    "    vocab_count = Counter([oo for o in words for oo in o])\n",
    "    vocab = [o for o, v in dict(vocab_count).items() if v>=3] + ['#UNK#']\n",
    "    word2index = defaultdict(int)\n",
    "    for i,o in enumerate(vocab): word2index[o] = i+1\n",
    "    # text to int\n",
    "    x = [[word2index[oo] for oo in o] for o in words]\n",
    "    y = [[tag2index[oo] for oo in o] for o in labels]\n",
    "    # training emission\n",
    "    emission_matrix = emission(x, y, vocab, tags)\n",
    "    # emission_matrix += 1e-5 Adding this smoothing will increase performance\n",
    "    print(\"emission_matrix shape:\", emission_matrix.shape)\n",
    "    # training transition\n",
    "    transition_matrix = transition(y, tags, tag2index)\n",
    "    print(\"transition_matrix shape\", transition_matrix.shape)\n",
    "    # decoding\n",
    "    viterbi_decode_batch(x, transition_matrix, emission_matrix,\n",
    "                         output_filename, dev_x_filename, word2index, tags)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (2698, 44)\n",
      "transition_matrix shape (43, 43)\n",
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 8520\n",
      "\n",
      "#Correct Entity : 6731\n",
      "Entity  precision: 0.7900\n",
      "Entity  recall: 0.8005\n",
      "Entity  F: 0.7953\n",
      "\n",
      "#Correct Sentiment : 6077\n",
      "Sentiment  precision: 0.7133\n",
      "Sentiment  recall: 0.7228\n",
      "Sentiment  F: 0.7180\n"
     ]
    }
   ],
   "source": [
    "q4(AL_train, AL_out_4, AL_dev_x)\n",
    "! python {EVAL_script} {AL_dev_y} {AL_out_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_matrix shape: (6187, 23)\n",
      "transition_matrix shape (22, 22)\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 12761\n",
      "\n",
      "#Correct Entity : 10785\n",
      "Entity  precision: 0.8452\n",
      "Entity  recall: 0.8183\n",
      "Entity  F: 0.8315\n",
      "\n",
      "#Correct Sentiment : 10370\n",
      "Sentiment  precision: 0.8126\n",
      "Sentiment  recall: 0.7869\n",
      "Sentiment  F: 0.7995\n"
     ]
    }
   ],
   "source": [
    "q4(EN_train, EN_out_4, EN_dev_x)\n",
    "! python {EVAL_script} {EN_dev_y} {EN_out_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
